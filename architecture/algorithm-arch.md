# IntelliKnow KMS â€” Algorithm Architecture

## 1. Document Ingestion Pipeline

### Overview
PDF/DOCX â†’ Text Chunks â†’ Embeddings â†’ FAISS Index

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Uploaded    â”‚    â”‚  LangChain      â”‚    â”‚  RecursiveCharacter         â”‚
â”‚  PDF/DOCX    â”‚â”€â”€â”€â–¶â”‚  Loader         â”‚â”€â”€â”€â–¶â”‚  TextSplitter               â”‚
â”‚  file        â”‚    â”‚  (extract text) â”‚    â”‚  chunk_size=500, overlap=50 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚  List[str] chunks
                                                       â–¼
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚  OpenAI Embeddings API    â”‚
                                           â”‚  text-embedding-3-small   â”‚
                                           â”‚  1536-dim float vectors   â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚  List[ndarray]
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚  FAISS IndexFlatL2         â”‚
                                           â”‚  Per intent-space index    â”‚
                                           â”‚  Persist to disk after add â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Chunking Strategy
- **Splitter**: `RecursiveCharacterTextSplitter`
- **chunk_size**: 500 characters (balances context vs. precision)
- **chunk_overlap**: 50 characters (prevents losing context at boundaries)
- **Separators**: `["\n\n", "\n", ". ", " ", ""]` â€” tries paragraph â†’ sentence â†’ word

### Embedding Model
- **Model**: `text-embedding-3-small` (1536 dimensions)
- **Batch size**: 20 chunks per API call to avoid rate limits
- **Cost estimate**: ~$0.002 per 1M tokens â€” negligible for MVP

### FAISS Configuration
- **Index type**: `IndexFlatL2` (exact L2 distance, no approximation)
- **Justification**: MVP has < 100 documents; exact search is fast enough
- **Persistence**: After each document addition, index is saved to `data/faiss/intent_{id}.index`

---

## 2. Intent Classification

### Overview
User query â†’ OpenAI zero-shot classification â†’ intent space + confidence score

```
User Query: "What is the reimbursement process?"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYSTEM PROMPT (built dynamically from DB):                        â”‚
â”‚                                                                     â”‚
â”‚  You are an intent classifier for an enterprise knowledge base.    â”‚
â”‚  Classify the user query into one of these intent spaces:          â”‚
â”‚                                                                     â”‚
â”‚  - HR: Human resources policies, employee handbook, ...            â”‚
â”‚    Keywords: HR, employee, leave, salary, policy                   â”‚
â”‚  - Legal: Contract templates, compliance policies, ...             â”‚
â”‚    Keywords: contract, compliance, legal, NDA                      â”‚
â”‚  - Finance: Expense reimbursement, budget policy, ...              â”‚
â”‚    Keywords: budget, expense, reimbursement                        â”‚
â”‚                                                                     â”‚
â”‚  Return JSON: {"intent": "<name>", "confidence": <0.0-1.0>}        â”‚
â”‚  If confidence < 0.7, set intent to "general".                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼  gpt-3.5-turbo (max_tokens=100, temperature=0)
        â”‚
        â–¼
{"intent": "Finance", "confidence": 0.88}
```

### Confidence Threshold
- **Default**: 0.7 (configurable via environment variable `INTENT_CONFIDENCE_THRESHOLD`)
- **Below threshold**: route to `general` â€” search all intent spaces combined
- **Rationale**: 0.7 is the minimum specified in requirements (FR-03-3)

### Fallback Behavior
1. confidence < 0.7 â†’ `general` routing â†’ search merged FAISS index (all spaces)
2. FAISS results similarity score < 0.5 â†’ "no relevant content" response
3. Knowledge base empty â†’ "KB is empty" response

---

## 3. RAG Retrieval Pipeline

### Overview
Query â†’ Embed â†’ FAISS search â†’ Assemble context â†’ LLM generate â†’ Cited response

```
User Query
    â”‚
    â–¼ OpenAI text-embedding-3-small (~0.2s)
Query Embedding (1536-dim vector)
    â”‚
    â–¼ FAISS IndexFlatL2.search(k=5)  (<50ms)
Top-5 chunks [with L2 distance scores]
    â”‚
    â–¼ Filter: distance > threshold (similarity < 0.5) â†’ fallback
Relevant chunks (1-5 items)
    â”‚
    â–¼ Assemble context string:
    â”‚   "Context:\n[chunk1]\n---\n[chunk2]\n..."
    â”‚
    â–¼ gpt-3.5-turbo (~1.5s)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYSTEM: You are a helpful knowledge base assistant.            â”‚
â”‚  Answer ONLY based on the provided context.                     â”‚
â”‚  If the answer is not in the context, say so clearly.          â”‚
â”‚  Keep answers concise (â‰¤200 words). Always cite source docs.   â”‚
â”‚                                                                  â”‚
â”‚  USER: {query}                                                   â”‚
â”‚                                                                  â”‚
â”‚  Context:                                                        â”‚
â”‚  {assembled_chunks}                                              â”‚
â”‚                                                                  â”‚
â”‚  Sources: {source_document_names}                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Response text + source document list
    â”‚
    â–¼ Format for channel (Telegram / Teams / API)
Final response
```

### Similarity Scoring
FAISS returns L2 distance. Lower = more similar.
- L2 distance is converted to a similarity score: `similarity = 1 / (1 + distance)`
- Threshold: if best chunk's similarity < 0.5, treat as "no relevant content found"

### Top-K Selection
- Retrieve k=5 chunks; pass all to LLM context
- LLM is instructed to synthesize from all relevant chunks, not just the first

---

## 4. Response Formatting

Responses are formatted differently per channel:

**Telegram** (plain text + emoji):
```
{answer_text}

ğŸ“„ Source: {filename1}, {filename2}
```

**Teams** (plain text; Adaptive Card format is optional P2):
```
{answer_text}

Source: {filename1}, {filename2}
```

**API** (raw â€” for Admin UI preview):
```json
{
  "answer": "...",
  "source_documents": ["filename1.pdf"]
}
```

---

## 5. Bot Integration Architecture

### Telegram (Polling)
```
TelegramBot.run_polling()
    â”‚  (runs in background thread via asyncio)
    â”‚
    â–¼ on_message(update)
    â”œâ”€â”€ extract text + user_id
    â”œâ”€â”€ POST /api/query {query, channel="telegram", user_id}
    â””â”€â”€ send reply via telegram.Bot.send_message()
```

No Webhook needed for local demo. `python-telegram-bot` v20 handles polling natively.

### Microsoft Teams (Bot Framework)
```
Teams Bot endpoint: POST /api/integrations/teams/messages
    â”‚
    â–¼ botbuilder-python processes Activity
    â”œâ”€â”€ extract text from activity.text
    â”œâ”€â”€ POST /api/query {query, channel="teams", user_id}
    â””â”€â”€ return Activity reply
```

Local testing: Bot Framework Emulator connects to `http://localhost:8000/api/integrations/teams/messages`

---

## 6. Performance Budget

| Operation | Target | Typical |
|-----------|--------|---------|
| Intent classification | < 800ms | ~400-600ms |
| Query embedding | < 300ms | ~200ms |
| FAISS search (k=5, <100 docs) | < 50ms | <5ms |
| LLM response generation | < 2000ms | ~1200-1800ms |
| **Total end-to-end** | **â‰¤ 3000ms** | **~2000-2500ms** |
| Document parsing (10MB PDF) | < 60s | ~5-15s |
| Embedding (50 chunks) | < 5s | ~2-3s |
